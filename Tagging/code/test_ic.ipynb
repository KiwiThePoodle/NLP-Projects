{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["#!/usr/bin/env python3"]},{"cell_type":"markdown","metadata":{},"source":["This file illustrates how you might experiment with the HMM interface.\n","You can paste these commands in at the Python prompt, or execute `test_ic.py` directly.\n","A notebook interface is nicer than the plain Python prompt, so we provide\n","a notebook version of this file as `test_ic.ipynb`, which you can open with\n","`jupyter` or with Visual Studio `code` (run it with the `nlp-class` kernel)."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import logging, math, os\n","from pathlib import Path\n","from typing import Callable"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/jamesyu/miniconda3/envs/nlp-class/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from corpus import TaggedCorpus\n","from eval import model_cross_entropy, write_tagging\n","from hmm import HiddenMarkovModel\n","from lexicon import build_lexicon\n","import torch"]},{"cell_type":"markdown","metadata":{},"source":["Set up logging."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["log = logging.getLogger(\"test_ic\")       # For usage, see findsim.py in earlier assignment.\n","logging.basicConfig(level=logging.INFO)  # could change INFO to DEBUG\n","# torch.autograd.set_detect_anomaly(True)    # uncomment to improve error messages from .backward(), but slows down"]},{"cell_type":"markdown","metadata":{},"source":["Switch working directory to the directory where the data live.  You may want to edit this line."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["os.chdir(\"../data\")"]},{"cell_type":"markdown","metadata":{},"source":["Make an HMM with randomly initialized parameters."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:corpus:Read 40 tokens from icsup\n","INFO:corpus:Created 4 tag types\n","INFO:corpus:Created 5 word types\n","INFO:test_ic:Ice cream vocabulary: ['1', '2', '3', '_EOS_WORD_', '_BOS_WORD_']\n","INFO:test_ic:Ice cream tagset: ['C', 'H', '_EOS_TAG_', '_BOS_TAG_']\n"]}],"source":["icsup = TaggedCorpus(Path(\"icsup\"), add_oov=False)\n","log.info(f\"Ice cream vocabulary: {list(icsup.vocab)}\")\n","log.info(f\"Ice cream tagset: {list(icsup.tagset)}\")\n","lexicon = build_lexicon(icsup, one_hot=True)   # one-hot lexicon: separate parameters for each word\n","hmm = HiddenMarkovModel(icsup.tagset, icsup.vocab, lexicon)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:test_ic:*** Current A, B matrices (computed by softmax from small random parameters)\n"]},{"name":"stdout","output_type":"stream","text":["Transition matrix A:\n","\tC\tH\t_EOS_TAG_\t_BOS_TAG_\n","C\t0.334\t0.332\t0.334\t0.000\n","H\t0.332\t0.334\t0.334\t0.000\n","_EOS_TAG_\t0.333\t0.335\t0.332\t0.000\n","_BOS_TAG_\t0.333\t0.333\t0.334\t0.000\n","\n","Emission matrix B:\n","\t1\t2\t3\n","C\t0.332\t0.334\t0.334\n","H\t0.332\t0.334\t0.334\n","_EOS_TAG_\t0.000\t0.000\t0.000\n","_BOS_TAG_\t0.000\t0.000\t0.000\n","\n","\n"]}],"source":["log.info(\"*** Current A, B matrices (computed by softmax from small random parameters)\")\n","hmm.updateAB()   # compute the matrices from the initial parameters (this would normally happen during training).\n","                 # An alternative is to set them directly to some spreadsheet values you'd like to try.\n","hmm.printAB()"]},{"cell_type":"markdown","metadata":{},"source":["While training on ice cream, we will just evaluate the cross-entropy\n","on the training data itself (icsup), since we are interested in watching it improve."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:test_ic:*** Supervised training on icsup\n","INFO:hmm:Training HiddenMarkovModel with 36 parameters\n","4it [00:00, 611.99it/s]\n","INFO:eval:Cross-entropy: 1.9662 nats (= perplexity 7.143)\n","28it [00:00, 277.34it/s]"]},{"name":"stdout","output_type":"stream","text":["1.9661612944169478\n"]},{"name":"stderr","output_type":"stream","text":["4it [00:00, 667.35it/s]s]\n","INFO:eval:Cross-entropy: 1.1094 nats (= perplexity 3.032)\n","532it [00:02, 285.96it/s]"]},{"name":"stdout","output_type":"stream","text":["1.1093741330233486\n"]},{"name":"stderr","output_type":"stream","text":["4it [00:00, 658.96it/s]s]\n","INFO:eval:Cross-entropy: 1.0949 nats (= perplexity 2.989)\n","1033it [00:03, 296.72it/s]"]},{"name":"stdout","output_type":"stream","text":["1.0948585163463245\n"]},{"name":"stderr","output_type":"stream","text":["4it [00:00, 760.18it/s]/s]\n","INFO:eval:Cross-entropy: 1.0893 nats (= perplexity 2.972)\n","1533it [00:05, 221.80it/s]"]},{"name":"stdout","output_type":"stream","text":["1.0893084352666682\n"]},{"name":"stderr","output_type":"stream","text":["4it [00:00, 444.59it/s]/s]\n","INFO:eval:Cross-entropy: 1.0864 nats (= perplexity 2.964)\n","2019it [00:08, 192.02it/s]"]},{"name":"stdout","output_type":"stream","text":["1.086406491019509\n"]},{"name":"stderr","output_type":"stream","text":["4it [00:00, 411.49it/s]/s]\n","INFO:eval:Cross-entropy: 1.0846 nats (= perplexity 2.958)\n","2521it [00:10, 194.33it/s]"]},{"name":"stdout","output_type":"stream","text":["1.0846293622797185\n"]},{"name":"stderr","output_type":"stream","text":["4it [00:00, 591.81it/s]/s]\n","INFO:eval:Cross-entropy: 1.0834 nats (= perplexity 2.955)\n","3029it [00:12, 222.55it/s]"]},{"name":"stdout","output_type":"stream","text":["1.0834312438964844\n"]},{"name":"stderr","output_type":"stream","text":["4it [00:00, 815.50it/s]/s]\n","INFO:eval:Cross-entropy: 1.0826 nats (= perplexity 2.952)\n","3529it [00:14, 254.85it/s]"]},{"name":"stdout","output_type":"stream","text":["1.0825699025934392\n"]},{"name":"stderr","output_type":"stream","text":["4it [00:00, 639.25it/s]/s]\n","INFO:eval:Cross-entropy: 1.0819 nats (= perplexity 2.950)\n","4045it [00:16, 239.71it/s]"]},{"name":"stdout","output_type":"stream","text":["1.08192101391879\n"]},{"name":"stderr","output_type":"stream","text":["4it [00:00, 653.47it/s]/s]\n","INFO:eval:Cross-entropy: 1.0814 nats (= perplexity 2.949)\n","4550it [00:18, 272.24it/s]"]},{"name":"stdout","output_type":"stream","text":["1.0814150029962712\n"]},{"name":"stderr","output_type":"stream","text":["4it [00:00, 664.68it/s]/s]\n","INFO:eval:Cross-entropy: 1.0810 nats (= perplexity 2.948)\n","5029it [00:20, 270.88it/s]"]},{"name":"stdout","output_type":"stream","text":["1.0810093879699707\n"]},{"name":"stderr","output_type":"stream","text":["4it [00:00, 687.42it/s]/s]\n","INFO:eval:Cross-entropy: 1.0807 nats (= perplexity 2.947)\n","5525it [00:22, 232.76it/s]"]},{"name":"stdout","output_type":"stream","text":["1.080676945773038\n"]},{"name":"stderr","output_type":"stream","text":["4it [00:00, 424.55it/s]/s]\n","INFO:eval:Cross-entropy: 1.0804 nats (= perplexity 2.946)\n","6012it [00:24, 209.55it/s]"]},{"name":"stdout","output_type":"stream","text":["1.0803996432911267\n"]},{"name":"stderr","output_type":"stream","text":["4it [00:00, 394.20it/s]/s]\n","INFO:eval:Cross-entropy: 1.0802 nats (= perplexity 2.945)\n","6531it [00:26, 201.42it/s]"]},{"name":"stdout","output_type":"stream","text":["1.0801649527116255\n"]},{"name":"stderr","output_type":"stream","text":["4it [00:00, 578.23it/s]/s]\n","INFO:eval:Cross-entropy: 1.0800 nats (= perplexity 2.945)\n","7031it [00:29, 228.59it/s]"]},{"name":"stdout","output_type":"stream","text":["1.0799635973843662\n"]},{"name":"stderr","output_type":"stream","text":["4it [00:00, 582.14it/s]/s]\n","INFO:eval:Cross-entropy: 1.0798 nats (= perplexity 2.944)\n","7536it [00:31, 242.74it/s]"]},{"name":"stdout","output_type":"stream","text":["1.079788944937966\n"]},{"name":"stderr","output_type":"stream","text":["4it [00:00, 496.03it/s]/s]\n","INFO:eval:Cross-entropy: 1.0796 nats (= perplexity 2.944)\n","8019it [00:33, 201.02it/s]"]},{"name":"stdout","output_type":"stream","text":["1.079636270349676\n"]},{"name":"stderr","output_type":"stream","text":["4it [00:00, 626.88it/s]/s]\n","INFO:eval:Cross-entropy: 1.0795 nats (= perplexity 2.943)\n","8541it [00:36, 227.35it/s]"]},{"name":"stdout","output_type":"stream","text":["1.0795014121315696\n"]},{"name":"stderr","output_type":"stream","text":["4it [00:00, 511.05it/s]/s]\n","INFO:eval:Cross-entropy: 1.0794 nats (= perplexity 2.943)\n","9033it [00:38, 222.75it/s]"]},{"name":"stdout","output_type":"stream","text":["1.0793814659118652\n"]},{"name":"stderr","output_type":"stream","text":["4it [00:00, 682.83it/s]/s]\n","INFO:eval:Cross-entropy: 1.0793 nats (= perplexity 2.943)\n","INFO:hmm:Saving model to my_hmm.pkl\n","INFO:hmm:Saved model to my_hmm.pkl\n","9500it [00:40, 235.72it/s]"]},{"name":"stdout","output_type":"stream","text":["1.0792743075977673\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["log.info(\"*** Supervised training on icsup\")\n","cross_entropy_loss = lambda model: model_cross_entropy(model, icsup)\n","hmm.train(corpus=icsup, loss=cross_entropy_loss, \n","          minibatch_size=10, evalbatch_size=500, lr=0.01, tolerance=0.0001)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:test_ic:*** A, B matrices after training on icsup (should approximately match initial params on spreadsheet [transposed])\n"]},{"name":"stdout","output_type":"stream","text":["Transition matrix A:\n","\tC\tH\t_EOS_TAG_\t_BOS_TAG_\n","C\t0.888\t0.110\t0.002\t0.000\n","H\t0.110\t0.888\t0.002\t0.000\n","_EOS_TAG_\t0.333\t0.335\t0.332\t0.000\n","_BOS_TAG_\t0.496\t0.496\t0.007\t0.000\n","\n","Emission matrix B:\n","\t1\t2\t3\n","C\t0.700\t0.200\t0.100\n","H\t0.100\t0.200\t0.700\n","_EOS_TAG_\t0.000\t0.000\t0.000\n","_BOS_TAG_\t0.000\t0.000\t0.000\n","\n","\n"]}],"source":["log.info(\"*** A, B matrices after training on icsup (should approximately \"\n","         \"match initial params on spreadsheet [transposed])\")\n","hmm.printAB()"]},{"cell_type":"markdown","metadata":{},"source":["Since we used a low tolerance, that should have gotten us about up to the\n","initial parameters on the spreadsheet.  Let's tag the spreadsheet \"sentence\"\n","(that is, the sequence of ice creams) using the Viterbi algorithm."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:test_ic:*** Viterbi results on icraw\n","1it [00:00, 56.06it/s]"]},{"name":"stdout","output_type":"stream","text":["[('_BOS_WORD_', '_BOS_TAG_'), ('2', 'H'), ('3', 'H'), ('3', 'H'), ('2', 'H'), ('3', 'H'), ('2', 'H'), ('3', 'H'), ('2', 'H'), ('2', 'H'), ('3', 'H'), ('1', 'H'), ('3', 'H'), ('3', 'H'), ('1', 'C'), ('1', 'C'), ('1', 'C'), ('2', 'C'), ('1', 'C'), ('1', 'C'), ('1', 'C'), ('3', 'C'), ('1', 'C'), ('2', 'C'), ('1', 'C'), ('1', 'C'), ('1', 'C'), ('2', 'C'), ('3', 'H'), ('3', 'H'), ('2', 'H'), ('3', 'H'), ('2', 'H'), ('2', 'H'), ('_EOS_WORD_', '_EOS_TAG_')]\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"data":{"text/plain":["0"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["log.info(\"*** Viterbi results on icraw\")\n","icraw = TaggedCorpus(Path(\"icraw\"), tagset=icsup.tagset, vocab=icsup.vocab)\n","write_tagging(hmm, icraw, Path(\"icraw.output\"))  # calls hmm.viterbi_tagging on each sentence\n","os.system(\"cat icraw.output\")   # print the file we just created, and remove it"]},{"cell_type":"markdown","metadata":{},"source":["Now let's use the forward algorithm to see what the model thinks about \n","the probability of the spreadsheet \"sentence.\""]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:test_ic:*** Forward algorithm on icraw (should approximately match iteration 0 on spreadsheet)\n","INFO:test_ic:2.503347234953789e-16 = p(2 3 3 2 3 2 3 2 2 3 1 3 3 1 1 1 2 1 1 1 3 1 2 1 1 1 2 3 3 2 3 2 2)\n"]}],"source":["log.info(\"*** Forward algorithm on icraw (should approximately match iteration 0 \"\n","             \"on spreadsheet)\")\n","for sentence in icraw:\n","    prob = math.exp(hmm.log_prob(sentence, icraw))\n","    log.info(f\"{prob} = p({sentence})\")"]},{"cell_type":"markdown","metadata":{},"source":["Finally, let's reestimate on the icraw data, as the spreadsheet does."]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:test_ic:*** Reestimating on icraw (perplexity should improve on every iteration)\n","INFO:hmm:Training HiddenMarkovModel with 36 parameters\n","1it [00:00, 115.89it/s]\n","INFO:eval:Cross-entropy: 1.0566 nats (= perplexity 2.877)\n"]},{"name":"stdout","output_type":"stream","text":["1.0565803752225988\n"]},{"name":"stderr","output_type":"stream","text":["1it [00:00, 143.58it/s]]\n","INFO:eval:Cross-entropy: nan nats (= perplexity nan)\n","511it [00:07, 62.28it/s]"]},{"name":"stdout","output_type":"stream","text":["nan\n"]},{"name":"stderr","output_type":"stream","text":["1it [00:00, 164.23it/s]]\n","INFO:eval:Cross-entropy: nan nats (= perplexity nan)\n","1012it [00:15, 75.91it/s]"]},{"name":"stdout","output_type":"stream","text":["nan\n"]},{"name":"stderr","output_type":"stream","text":["1it [00:00, 138.93it/s]s]\n","INFO:eval:Cross-entropy: nan nats (= perplexity nan)\n","1512it [00:22, 74.27it/s]"]},{"name":"stdout","output_type":"stream","text":["nan\n"]},{"name":"stderr","output_type":"stream","text":["1it [00:00, 188.75it/s]s]\n","INFO:eval:Cross-entropy: nan nats (= perplexity nan)\n","2015it [00:28, 91.77it/s]"]},{"name":"stdout","output_type":"stream","text":["nan\n"]},{"name":"stderr","output_type":"stream","text":["1it [00:00, 136.65it/s]s]\n","INFO:eval:Cross-entropy: nan nats (= perplexity nan)\n","2508it [00:34, 63.82it/s]"]},{"name":"stdout","output_type":"stream","text":["nan\n"]},{"name":"stderr","output_type":"stream","text":["1it [00:00, 173.22it/s]s]\n","INFO:eval:Cross-entropy: nan nats (= perplexity nan)\n","3017it [00:40, 81.98it/s]"]},{"name":"stdout","output_type":"stream","text":["nan\n"]},{"name":"stderr","output_type":"stream","text":["1it [00:00, 96.07it/s]/s]\n","INFO:eval:Cross-entropy: nan nats (= perplexity nan)\n","3509it [00:47, 46.69it/s]"]},{"name":"stdout","output_type":"stream","text":["nan\n"]},{"name":"stderr","output_type":"stream","text":["1it [00:00, 175.97it/s]s]\n","INFO:eval:Cross-entropy: nan nats (= perplexity nan)\n","4017it [00:54, 78.88it/s]"]},{"name":"stdout","output_type":"stream","text":["nan\n"]},{"name":"stderr","output_type":"stream","text":["1it [00:00, 203.99it/s]s]\n","INFO:eval:Cross-entropy: nan nats (= perplexity nan)\n","4515it [01:00, 84.30it/s]"]},{"name":"stdout","output_type":"stream","text":["nan\n"]},{"name":"stderr","output_type":"stream","text":["1it [00:00, 162.47it/s]s]\n","INFO:eval:Cross-entropy: nan nats (= perplexity nan)\n","5011it [01:06, 80.22it/s]"]},{"name":"stdout","output_type":"stream","text":["nan\n"]},{"name":"stderr","output_type":"stream","text":["1it [00:00, 145.52it/s]s]\n","INFO:eval:Cross-entropy: nan nats (= perplexity nan)\n","5508it [01:14, 74.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["nan\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/Users/jamesyu/Downloads/hw-tag/code/test_ic.ipynb Cell 20\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jamesyu/Downloads/hw-tag/code/test_ic.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m log\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39m*** Reestimating on icraw (perplexity should improve on every iteration)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jamesyu/Downloads/hw-tag/code/test_ic.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m negative_log_likelihood \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m model: model_cross_entropy(model, icraw)  \u001b[39m# evaluate on icraw itself\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jamesyu/Downloads/hw-tag/code/test_ic.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m hmm\u001b[39m.\u001b[39;49mtrain(corpus\u001b[39m=\u001b[39;49micraw, loss\u001b[39m=\u001b[39;49mnegative_log_likelihood,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jamesyu/Downloads/hw-tag/code/test_ic.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m           minibatch_size\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, evalbatch_size\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m, lr\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m, tolerance\u001b[39m=\u001b[39;49m\u001b[39m0.0001\u001b[39;49m)\n","File \u001b[0;32m~/Downloads/hw-tag/code/hmm.py:362\u001b[0m, in \u001b[0;36mHiddenMarkovModel.train\u001b[0;34m(self, corpus, loss, tolerance, minibatch_size, evalbatch_size, lr, reg, save_path)\u001b[0m\n\u001b[1;32m    359\u001b[0m     old_dev_loss \u001b[39m=\u001b[39m dev_loss            \u001b[39m# remember for next eval batch\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[39m# Finally, add likelihood of sentence m to the minibatch objective.\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m log_likelihood \u001b[39m=\u001b[39m log_likelihood \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_prob(sentence, corpus)\n","File \u001b[0;32m~/Downloads/hw-tag/code/hmm.py:192\u001b[0m, in \u001b[0;36mlog_prob\u001b[0;34m(self, sentence, corpus)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[39m@typechecked\u001b[39m\n\u001b[1;32m    185\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_prob\u001b[39m(\u001b[39mself\u001b[39m, sentence: Sentence, corpus: TaggedCorpus) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m TorchScalar:\n\u001b[1;32m    186\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the log probability of a single sentence under the current\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39m    model parameters.  If the sentence is not fully tagged, the probability\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[39m    will marginalize over all possible tags.  \u001b[39;00m\n\u001b[1;32m    189\u001b[0m \n\u001b[1;32m    190\u001b[0m \u001b[39m    When the logging level is set to DEBUG, the alpha and beta vectors and posterior counts\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39m    are logged.  You can check this against the ice cream spreadsheet.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_forward(sentence, corpus)\n","File \u001b[0;32m~/Downloads/hw-tag/code/hmm.py:239\u001b[0m, in \u001b[0;36mlog_forward\u001b[0;34m(self, sentence, corpus)\u001b[0m\n\u001b[1;32m    237\u001b[0m         alpha[j][t_j] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlogsumexp(alpha[j \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mlog(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA[:, t_j]) \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mlog(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mB[t_j, w_j]), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, safe_inf\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    238\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m         alpha[j] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlogsumexp(alpha[j \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mlog(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA) \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mlog(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mB[:, w_j]\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, safe_inf\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    240\u001b[0m log_Z \u001b[39m=\u001b[39m logsumexp_new(alpha[length \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA[:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meos_t], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, safe_inf\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    241\u001b[0m \u001b[39mreturn\u001b[39;00m log_Z\n","File \u001b[0;32m~/Downloads/hw-tag/code/logsumexp_safe.py:148\u001b[0m, in \u001b[0;36mlogsumexp_new\u001b[0;34m(x, dim, keepdim, safe_inf)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Modified version of the standard torch.logsumexp.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[39mIf `safe_inf=True` is specified, it will try to avoid nans\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[39min the backward pass when the result is ±∞.\"\"\"\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m safe_inf:\n\u001b[0;32m--> 148\u001b[0m     result \u001b[39m=\u001b[39m LogSumExp_safe_inf\u001b[39m.\u001b[39;49mapply(x, dim, keepdim)\n\u001b[1;32m    149\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m     result \u001b[39m=\u001b[39m  torch\u001b[39m.\u001b[39mlogsumexp_old(x, dim, keepdim\u001b[39m=\u001b[39mkeepdim)\n","File \u001b[0;32m~/Downloads/hw-tag/code/logsumexp_safe.py:92\u001b[0m, in \u001b[0;36mLogSumExp_safe_inf.forward\u001b[0;34m(ctx, input, dim, keepdim)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(ctx, \u001b[39minput\u001b[39m, dim, keepdim \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m): \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m---> 92\u001b[0m         output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mlogsumexp_old(\u001b[39minput\u001b[39;49m, dim, keepdim\u001b[39m=\u001b[39;49mkeepdim) \u001b[39m# internal copy of output\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     ctx\u001b[39m.\u001b[39msave_for_backward(\u001b[39minput\u001b[39m, output)\n\u001b[1;32m     94\u001b[0m     ctx\u001b[39m.\u001b[39mdim \u001b[39m=\u001b[39m dim\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["log.info(\"*** Reestimating on icraw (perplexity should improve on every iteration)\")\n","negative_log_likelihood = lambda model: model_cross_entropy(model, icraw)  # evaluate on icraw itself\n","hmm.train(corpus=icraw, loss=negative_log_likelihood,\n","          minibatch_size=10, evalbatch_size=500, lr=0.001, tolerance=0.0001)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["log.info(\"*** A, B matrices after reestimation on icraw (SGD, not EM, but still \"\n","         \"should approximately match final params on spreadsheet [transposed])\")\n","hmm.printAB()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":2}
