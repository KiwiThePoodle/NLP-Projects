{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["#!/usr/bin/env python3"]},{"cell_type":"markdown","metadata":{},"source":["This file illustrates how you might experiment with the HMM interface.\n","You can paste these commands in at the Python prompt, or execute `test_en.py` directly.\n","A notebook interface is nicer than the plain Python prompt, so we provide\n","a notebook version of this file as `test_en.ipynb`, which you can open with\n","`jupyter` or with Visual Studio `code` (run it with the `nlp-class` kernel)."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import logging\n","import math\n","import os\n","from pathlib import Path\n","from typing import Callable"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/jamesyu/miniconda3/envs/nlp-class/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from corpus import TaggedCorpus\n","from eval import eval_tagging, model_cross_entropy, viterbi_error_rate\n","from hmm import HiddenMarkovModel\n","from lexicon import build_lexicon\n","import torch"]},{"cell_type":"markdown","metadata":{},"source":["Set up logging."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["log = logging.getLogger(\"test_en\")       # For usage, see findsim.py in earlier assignment.\n","logging.basicConfig(format=\"%(levelname)s : %(message)s\", level=logging.INFO)  # could change INFO to DEBUG\n","# torch.autograd.set_detect_anomaly(True)    # uncomment to improve error messages from .backward(), but slows down"]},{"cell_type":"markdown","metadata":{},"source":["Switch working directory to the directory where the data live.  You may need to edit this line."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["os.chdir(\"../data\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO : Read 191873 tokens from ensup, enraw\n","INFO : Created 26 tag types\n","INFO : Created 18461 word types\n","INFO : Tagset: f['W', 'J', 'N', 'C', 'V', 'I', 'D', ',', 'M', 'P', '.', 'E', 'R', '`', \"'\", 'T', '$', ':', '-', '#', 'S', 'F', 'U', 'L', '_EOS_TAG_', '_BOS_TAG_']\n","INFO : Read 95936 tokens from ensup\n","INFO : Created 26 tag types\n","INFO : Created 12466 word types\n"]}],"source":["entrain = TaggedCorpus(Path(\"ensup\"), Path(\"enraw\"))                               # all training\n","ensup =   TaggedCorpus(Path(\"ensup\"), tagset=entrain.tagset, vocab=entrain.vocab)  # supervised training\n","endev =   TaggedCorpus(Path(\"endev\"), tagset=entrain.tagset, vocab=entrain.vocab)  # evaluation\n","log.info(f\"Tagset: f{list(entrain.tagset)}\")\n","known_vocab = TaggedCorpus(Path(\"ensup\")).vocab    # words seen with supervised tags; used in evaluation"]},{"cell_type":"markdown","metadata":{},"source":["Make an HMM.  Let's do supervised pre-training to approximately\n","maximize the regularized log-likelihood.  If you want to speed this\n","up, you can increase the tolerance of training (using the\n","`tolerance` argument), since we don't really have to train to\n","convergence.\n","\n","We arbitrarily choose `reg=1`, but it would be better to search\n","for the best regularization strength."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO : From words-50.txt, got embeddings for 10420 of 18461 word types\n","INFO : Training HiddenMarkovModel with 1976 parameters\n","4051it [00:07, 545.22it/s]\n","INFO : Cross-entropy: 12.5261 nats (= perplexity 275434.516)\n","1it [00:07,  7.62s/it]"]},{"name":"stdout","output_type":"stream","text":["12.526105188718478\n"]},{"name":"stderr","output_type":"stream","text":["4051it [00:06, 578.92it/s]\n","INFO : Cross-entropy: 8.8428 nats (= perplexity 6924.085)\n","10001it [02:44, 10.03it/s]"]},{"name":"stdout","output_type":"stream","text":["8.842761257247592\n"]},{"name":"stderr","output_type":"stream","text":["4051it [00:07, 563.27it/s]\n","INFO : Cross-entropy: 8.5613 nats (= perplexity 5225.534)\n","20001it [05:12, 10.94it/s]"]},{"name":"stdout","output_type":"stream","text":["8.561312288720398\n"]},{"name":"stderr","output_type":"stream","text":["4051it [00:06, 596.81it/s]\n","INFO : Cross-entropy: 8.4019 nats (= perplexity 4455.305)\n","30001it [07:43, 12.07it/s]"]},{"name":"stdout","output_type":"stream","text":["8.401850737013811\n"]},{"name":"stderr","output_type":"stream","text":["4051it [00:07, 508.69it/s]\n","INFO : Cross-entropy: 8.3053 nats (= perplexity 4045.106)\n","40001it [10:22,  7.51it/s]"]},{"name":"stdout","output_type":"stream","text":["8.305262947065351\n"]},{"name":"stderr","output_type":"stream","text":["4051it [00:07, 571.20it/s] \n","INFO : Cross-entropy: 8.2401 nats (= perplexity 3790.021)\n","50001it [12:51, 11.19it/s]"]},{"name":"stdout","output_type":"stream","text":["8.240126956460243\n"]},{"name":"stderr","output_type":"stream","text":["4051it [00:06, 592.70it/s]\n","INFO : Cross-entropy: 8.1914 nats (= perplexity 3609.623)\n","60001it [15:14, 11.68it/s]"]},{"name":"stdout","output_type":"stream","text":["8.191358508598459\n"]},{"name":"stderr","output_type":"stream","text":["4051it [00:06, 652.33it/s]\n","INFO : Cross-entropy: 8.1560 nats (= perplexity 3484.173)\n","70001it [17:38, 11.27it/s]"]},{"name":"stdout","output_type":"stream","text":["8.155985990897781\n"]},{"name":"stderr","output_type":"stream","text":["4051it [00:07, 540.60it/s] \n","INFO : Cross-entropy: 8.1244 nats (= perplexity 3375.717)\n","80001it [19:57, 10.56it/s]"]},{"name":"stdout","output_type":"stream","text":["8.124363016004184\n"]},{"name":"stderr","output_type":"stream","text":["4051it [00:06, 646.20it/s]\n","INFO : Cross-entropy: 8.0995 nats (= perplexity 3292.720)\n","90001it [22:18, 13.33it/s]"]},{"name":"stdout","output_type":"stream","text":["8.099469246118877\n"]},{"name":"stderr","output_type":"stream","text":["4051it [00:07, 570.29it/s]\n","INFO : Cross-entropy: 8.0781 nats (= perplexity 3223.228)\n","100001it [24:33,  9.80it/s]"]},{"name":"stdout","output_type":"stream","text":["8.078138514234725\n"]},{"name":"stderr","output_type":"stream","text":["4051it [00:07, 515.94it/s]]\n","INFO : Cross-entropy: 8.0614 nats (= perplexity 3169.645)\n","110001it [26:55,  9.76it/s]"]},{"name":"stdout","output_type":"stream","text":["8.06137496290274\n"]},{"name":"stderr","output_type":"stream","text":["4051it [00:08, 457.09it/s]]\n","INFO : Cross-entropy: 8.0456 nats (= perplexity 3119.964)\n","120001it [29:33,  9.61it/s]"]},{"name":"stdout","output_type":"stream","text":["8.04557678471941\n"]},{"name":"stderr","output_type":"stream","text":["4051it [00:06, 594.85it/s]]\n","INFO : Cross-entropy: 8.0326 nats (= perplexity 3079.783)\n","130001it [32:09, 10.09it/s]"]},{"name":"stdout","output_type":"stream","text":["8.03261450685748\n"]},{"name":"stderr","output_type":"stream","text":["4051it [00:07, 513.14it/s]]\n","INFO : Cross-entropy: 8.0181 nats (= perplexity 3035.365)\n","140001it [34:45, 10.03it/s]"]},{"name":"stdout","output_type":"stream","text":["8.018086800231936\n"]},{"name":"stderr","output_type":"stream","text":["4051it [00:06, 611.15it/s]]\n","INFO : Cross-entropy: 8.0106 nats (= perplexity 3012.711)\n","INFO : Saving model to ensup_hmm.pkl\n","INFO : Saved model to ensup_hmm.pkl\n","150000it [37:05, 67.41it/s]"]},{"name":"stdout","output_type":"stream","text":["8.010595584579315\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["lexicon = build_lexicon(entrain, embeddings_file=Path('words-50.txt'))  # works better with more dims!\n","hmm = HiddenMarkovModel(entrain.tagset, entrain.vocab, lexicon)  # randomly initialized parameters\n","loss_sup = lambda model: model_cross_entropy(model, eval_corpus=ensup)\n","hmm.train(corpus=ensup, loss=loss_sup, \n","          minibatch_size=30, evalbatch_size=10000, \n","          reg=1, lr=0.0001, save_path=\"ensup_hmm.pkl\") "]},{"cell_type":"markdown","metadata":{},"source":["Now let's throw in the unsupervised training data as well, and continue\n","training to try to improve accuracy on held-out development data.\n","We'll stop when this accuracy stops getting better.\n","\n","This step is delicate, so we'll use a much smaller learning rate and\n","pause to evaluate more often, in hopes that tagging accuracy will go\n","up for a little bit before it goes down again (see Merialdo 1994).\n","(Log-likelihood will continue to improve, just not accuracy.)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO : Loading model from ensup_hmm.pkl\n","INFO : Loaded model from ensup_hmm.pkl\n","INFO : Training HiddenMarkovModel with 1976 parameters\n","996it [00:02, 420.52it/s]\n","INFO : Cross-entropy: 8.0400 nats (= perplexity 3102.635)\n"]},{"name":"stdout","output_type":"stream","text":["8.040006991669514\n"]},{"name":"stderr","output_type":"stream","text":["996it [00:02, 359.64it/s]\n","INFO : Tagging accuracy: all: 81.739%, known: 83.175%, seen: 72.391%, novel: 62.814%\n","996it [00:02, 344.89it/s]\n","INFO : Cross-entropy: nan nats (= perplexity nan)\n"]},{"name":"stdout","output_type":"stream","text":["nan\n"]},{"name":"stderr","output_type":"stream","text":["996it [00:03, 331.56it/s]\n","INFO : Tagging accuracy: all: 4.664%, known: 5.073%, seen: 0.168%, novel: 0.000%\n","INFO : Saving model to entrain_hmm.pkl\n","INFO : Saved model to entrain_hmm.pkl\n","2016it [00:48, 41.65it/s]\n"]}],"source":["hmm = HiddenMarkovModel.load(\"ensup_hmm.pkl\")  # reset to supervised model (in case you're re-executing this bit)\n","loss_dev = lambda model: viterbi_error_rate(model, eval_corpus=endev, \n","                                            known_vocab=known_vocab)\n","hmm.train(corpus=entrain, loss=loss_dev,\n","          minibatch_size=30, evalbatch_size=len(entrain)//4, # evaluate 4 times per epoch\n","          reg=1, lr=0.000001, save_path=\"entrain_hmm.pkl\")"]},{"cell_type":"markdown","metadata":{},"source":["You can also retry the above workflow where you start with a worse\n","supervised model (like Merialdo).  Replace `ensup` throughout the\n","corpus setup with `ensup-tiny`, which is only 25 sentences (that\n","cover all tags in `endev`).  And change the names of your saved\n","models."]},{"cell_type":"markdown","metadata":{},"source":["More detailed look at the first 10 sentences in the held-out corpus,\n","including Viterbi tagging."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO : Gold:    ``/` We/P 're/V strongly/R _OOV_/V that/I anyone/N who/W has/V eaten/V in/I the/D cafeteria/N this/D month/N have/V the/D shot/N ,/, ''/' Mr./N Mattausch/N added/V ,/, ``/` and/C that/D means/V virtually/R everyone/N who/W works/V here/R ./.\n","INFO : Viterbi: [('_BOS_WORD_', 'W'), ('``', 'W'), ('We', 'W'), (\"'re\", 'W'), ('strongly', 'W'), ('_OOV_', 'W'), ('that', 'W'), ('anyone', 'W'), ('who', 'W'), ('has', 'W'), ('eaten', 'W'), ('in', 'W'), ('the', 'W'), ('cafeteria', 'W'), ('this', 'W'), ('month', 'W'), ('have', 'W'), ('the', 'W'), ('shot', 'W'), (',', 'W'), (\"''\", 'W'), ('Mr.', 'W'), ('Mattausch', 'W'), ('added', 'W'), (',', 'W'), ('``', 'W'), ('and', 'W'), ('that', 'W'), ('means', 'W'), ('virtually', 'W'), ('everyone', 'W'), ('who', 'W'), ('works', 'W'), ('here', 'W'), ('.', 'W'), ('_EOS_WORD_', '_EOS_TAG_')]\n","INFO : Loss:    33/36\n","INFO : Prob:    nan\n","INFO : Gold:    I/P was/V _OOV_/V to/T read/V the/D _OOV_/N of/I facts/N in/I your/P Oct./N 13/C editorial/N ``/` _OOV_/N 's/P _OOV_/N _OOV_/N ./. ''/'\n","INFO : Viterbi: [('_BOS_WORD_', 'W'), ('I', 'W'), ('was', 'W'), ('_OOV_', 'W'), ('to', 'W'), ('read', 'W'), ('the', 'W'), ('_OOV_', 'W'), ('of', 'W'), ('facts', 'W'), ('in', 'W'), ('your', 'W'), ('Oct.', 'W'), ('13', 'W'), ('editorial', 'W'), ('``', 'W'), ('_OOV_', 'W'), (\"'s\", 'W'), ('_OOV_', 'W'), ('_OOV_', 'W'), ('.', 'W'), (\"''\", 'W'), ('_EOS_WORD_', '_EOS_TAG_')]\n","INFO : Loss:    22/23\n","INFO : Prob:    nan\n","INFO : Gold:    It/P is/V the/D _OOV_/J guerrillas/N who/W are/V aligned/V with/I the/D drug/N traffickers/N ,/, not/R the/D left/J _OOV_/N ./.\n","INFO : Viterbi: [('_BOS_WORD_', 'W'), ('It', 'W'), ('is', 'W'), ('the', 'W'), ('_OOV_', 'W'), ('guerrillas', 'W'), ('who', 'W'), ('are', 'W'), ('aligned', 'W'), ('with', 'W'), ('the', 'W'), ('drug', 'W'), ('traffickers', 'W'), (',', 'W'), ('not', 'W'), ('the', 'W'), ('left', 'W'), ('_OOV_', 'W'), ('.', 'W'), ('_EOS_WORD_', '_EOS_TAG_')]\n","INFO : Loss:    18/20\n","INFO : Prob:    nan\n","INFO : Gold:    This/D information/N was/V _OOV_/V from/I your/P own/J news/N stories/N on/I the/D region/N ./.\n","INFO : Viterbi: [('_BOS_WORD_', 'W'), ('This', 'W'), ('information', 'W'), ('was', 'W'), ('_OOV_', 'W'), ('from', 'W'), ('your', 'W'), ('own', 'W'), ('news', 'W'), ('stories', 'W'), ('on', 'W'), ('the', 'W'), ('region', 'W'), ('.', 'W'), ('_EOS_WORD_', '_EOS_TAG_')]\n","INFO : Loss:    14/15\n","INFO : Prob:    nan\n","INFO : Gold:    _OOV_/J _OOV_/J government/N _OOV_/N of/I the/D ``/` _OOV_/F ''/' was/V due/J to/T the/D drug/N _OOV_/N '/P history/N of/I _OOV_/V out/R _OOV_/N in/I the/D _OOV_/N ./.\n","INFO : Viterbi: [('_BOS_WORD_', 'W'), ('_OOV_', 'W'), ('_OOV_', 'W'), ('government', 'W'), ('_OOV_', 'W'), ('of', 'W'), ('the', 'W'), ('``', 'W'), ('_OOV_', 'W'), (\"''\", 'W'), ('was', 'W'), ('due', 'W'), ('to', 'W'), ('the', 'W'), ('drug', 'W'), ('_OOV_', 'W'), (\"'\", 'W'), ('history', 'W'), ('of', 'W'), ('_OOV_', 'W'), ('out', 'W'), ('_OOV_', 'W'), ('in', 'W'), ('the', 'W'), ('_OOV_', 'W'), ('.', 'W'), ('_EOS_WORD_', '_EOS_TAG_')]\n","INFO : Loss:    26/27\n","INFO : Prob:    nan\n","INFO : Gold:    Mary/N _OOV_/N Palo/N Alto/N ,/, Calif/N ./.\n","INFO : Viterbi: [('_BOS_WORD_', 'W'), ('Mary', 'W'), ('_OOV_', 'W'), ('Palo', 'W'), ('Alto', 'W'), (',', 'W'), ('Calif', 'W'), ('.', 'W'), ('_EOS_WORD_', '_EOS_TAG_')]\n","INFO : Loss:    8/9\n","INFO : Prob:    nan\n","INFO : Gold:    I/P suggest/V that/I The/D Wall/N Street/N Journal/N -LRB-/- as/R well/R as/I other/J U.S./N news/N publications/N of/I like/J mind/N -RRB-/- should/M put/V its/P money/N where/W its/P mouth/N is/V :/: _OOV_/V computer/N equipment/N to/T replace/V that/I damaged/V at/I El/N _OOV_/N ,/, buy/V ad/N space/N ,/, publish/V stories/N under/I the/D _OOV_/N of/I El/N _OOV_/N journalists/N ./.\n","INFO : Viterbi: [('_BOS_WORD_', 'W'), ('I', 'W'), ('suggest', 'W'), ('that', 'W'), ('The', 'W'), ('Wall', 'W'), ('Street', 'W'), ('Journal', 'W'), ('-LRB-', 'W'), ('as', 'W'), ('well', 'W'), ('as', 'W'), ('other', 'W'), ('U.S.', 'W'), ('news', 'W'), ('publications', 'W'), ('of', 'W'), ('like', 'W'), ('mind', 'W'), ('-RRB-', 'W'), ('should', 'W'), ('put', 'W'), ('its', 'W'), ('money', 'W'), ('where', 'W'), ('its', 'W'), ('mouth', 'W'), ('is', 'W'), (':', 'W'), ('_OOV_', 'W'), ('computer', 'W'), ('equipment', 'W'), ('to', 'W'), ('replace', 'W'), ('that', 'W'), ('damaged', 'W'), ('at', 'W'), ('El', 'W'), ('_OOV_', 'W'), (',', 'W'), ('buy', 'W'), ('ad', 'W'), ('space', 'W'), (',', 'W'), ('publish', 'W'), ('stories', 'W'), ('under', 'W'), ('the', 'W'), ('_OOV_', 'W'), ('of', 'W'), ('El', 'W'), ('_OOV_', 'W'), ('journalists', 'W'), ('.', 'W'), ('_EOS_WORD_', '_EOS_TAG_')]\n","INFO : Loss:    53/55\n","INFO : Prob:    nan\n","INFO : Gold:    Perhaps/R an/D arrangement/N could/M be/V worked/V out/R to/T ``/` sponsor/V ''/' El/N _OOV_/N journalists/N and/C staff/N by/I paying/V for/I added/V security/N in/I exchange/N for/I exclusive/J stories/N ./.\n","INFO : Viterbi: [('_BOS_WORD_', 'W'), ('Perhaps', 'W'), ('an', 'W'), ('arrangement', 'W'), ('could', 'W'), ('be', 'W'), ('worked', 'W'), ('out', 'W'), ('to', 'W'), ('``', 'W'), ('sponsor', 'W'), (\"''\", 'W'), ('El', 'W'), ('_OOV_', 'W'), ('journalists', 'W'), ('and', 'W'), ('staff', 'W'), ('by', 'W'), ('paying', 'W'), ('for', 'W'), ('added', 'W'), ('security', 'W'), ('in', 'W'), ('exchange', 'W'), ('for', 'W'), ('exclusive', 'W'), ('stories', 'W'), ('.', 'W'), ('_EOS_WORD_', '_EOS_TAG_')]\n","INFO : Loss:    28/29\n","INFO : Prob:    nan\n","INFO : Gold:    _OOV_/V El/N _OOV_/N 's/P courage/N with/I real/J support/N ./.\n","INFO : Viterbi: [('_BOS_WORD_', 'W'), ('_OOV_', 'W'), ('El', 'W'), ('_OOV_', 'W'), (\"'s\", 'W'), ('courage', 'W'), ('with', 'W'), ('real', 'W'), ('support', 'W'), ('.', 'W'), ('_EOS_WORD_', '_EOS_TAG_')]\n","INFO : Loss:    10/11\n","INFO : Prob:    nan\n","INFO : Gold:    Douglas/N B./N Evans/N\n","INFO : Viterbi: [('_BOS_WORD_', 'W'), ('Douglas', 'W'), ('B.', 'W'), ('Evans', 'W'), ('_EOS_WORD_', '_EOS_TAG_')]\n","INFO : Loss:    4/5\n","INFO : Prob:    nan\n"]}],"source":["for m, sentence in enumerate(endev):\n","    if m >= 10: break\n","    viterbi = hmm.viterbi_tagging(sentence.desupervise(), endev)\n","    counts = eval_tagging(predicted=viterbi, gold=sentence, \n","                          known_vocab=known_vocab)\n","    num = counts['NUM', 'ALL']\n","    denom = counts['DENOM', 'ALL']\n","    \n","    log.info(f\"Gold:    {sentence}\")\n","    log.info(f\"Viterbi: {viterbi}\")\n","    log.info(f\"Loss:    {denom - num}/{denom}\")\n","    log.info(f\"Prob:    {math.exp(hmm.log_prob(sentence, endev))}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":2}
